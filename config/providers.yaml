# LLM Provider Configuration
# 
# This file configures LLM providers for the Skill-Governed Framework.
# Supports multiple providers for different use cases in complex experiments.

# =============================================================================
# PROVIDERS
# =============================================================================
providers:
  # Local models via Ollama (recommended for development/testing)
  local-llama:
    type: ollama
    model: llama3.2:3b
    base_url: http://localhost:11434
    temperature: 0.7
    max_tokens: 1024
  
  local-gemma:
    type: ollama
    model: gemma3:4b
    base_url: http://localhost:11434
    temperature: 0.7
    max_tokens: 1024
  
  local-deepseek:
    type: ollama
    model: deepseek-r1:8b
    base_url: http://localhost:11434
    temperature: 0.7
    max_tokens: 2048
  
  local-gpt-oss:
    type: ollama
    model: aya-expanse:8b
    base_url: http://localhost:11434
    temperature: 0.7
    max_tokens: 1024
  
  # Cloud models (for production/high-stakes decisions)
  cloud-gpt4:
    type: openai
    model: gpt-4-turbo
    api_key: ${OPENAI_API_KEY}
    temperature: 0.7
    max_tokens: 2048
  
  cloud-gpt35:
    type: openai
    model: gpt-3.5-turbo
    api_key: ${OPENAI_API_KEY}
    temperature: 0.7
    max_tokens: 1024

# =============================================================================
# DEFAULT PROVIDER
# =============================================================================
default: local-llama

# =============================================================================
# ROUTING RULES (for hybrid experiments)
# =============================================================================
# Use different providers based on context
routing:
  default: local-llama
  high_stakes: cloud-gpt4      # Important decisions → cloud
  validation: local-llama      # Validation retries → local
  batch: local-llama           # Batch processing → local

# Fallback when primary provider fails
fallback: local-llama

# =============================================================================
# EXPERIMENT PRESETS
# =============================================================================
# Quick presets for different experiment types
presets:
  # Single model experiment
  single_local:
    providers: [local-llama]
    default: local-llama
  
  # Multi-model comparison (current Exp 10)
  multi_model_comparison:
    providers: [local-llama, local-gemma, local-deepseek, local-gpt-oss]
    default: local-llama
  
  # Hybrid cloud+local
  hybrid:
    providers: [local-llama, cloud-gpt4]
    routing:
      default: local-llama
      high_stakes: cloud-gpt4
