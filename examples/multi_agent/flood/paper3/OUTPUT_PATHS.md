# Paper 3 Output File Paths

Complete specification of all output file paths for Paper 3 experiment and analysis.

## Directory Structure

```text
examples/multi_agent/flood/paper3/
│
├── analysis/                              # Analysis scripts
│   ├── figures/                          # All generated figures
│   │   ├── agent_spatial_distribution.png
│   │   ├── agent_spatial_distribution.pdf
│   │   ├── agent_spatial_distribution_panels.png
│   │   ├── agent_spatial_distribution_panels.pdf
│   │   ├── agent_distribution_stats.csv
│   │   ├── fig1_study_area.png           # (future)
│   │   ├── fig2_agent_types.png          # (future)
│   │   ├── fig3_spaghetti_plot.png       # RQ1: adaptation trajectories
│   │   ├── fig4_institutional_feedback.png # RQ2: inequality
│   │   └── fig5_network_diffusion.png    # RQ3: social info
│   │
│   ├── tables/                           # Generated tables for WRR
│   │   ├── table1_agent_demographics.csv
│   │   ├── table2_validation_summary.csv
│   │   ├── table3_benchmark_comparison.csv
│   │   └── table4_rq_results.csv
│   │
│   ├── fig_agent_spatial_distribution.py
│   ├── compute_validation_metrics.py
│   ├── persona_sensitivity.py
│   ├── prompt_sensitivity.py
│   ├── empirical_benchmarks.py
│   ├── generate_wrr_tables.py
│   └── generate_wrr_micro_tables.py
│
├── results/                              # All experiment/validation outputs
│   │
│   ├── cv/                               # L3 Cognitive Validation (pre-experiment)
│   │   ├── icc_report.json              # ICC(2,1), eta², summary
│   │   ├── icc_responses.csv            # 2,700 raw LLM responses
│   │   ├── icc_log.txt                  # Execution log
│   │   ├── persona_sensitivity_report.json
│   │   └── prompt_sensitivity_report.json
│   │
│   ├── validation/                       # L1/L2 Validation (post-experiment)
│   │   ├── validation_report.json       # Combined L1+L2 summary
│   │   ├── l1_micro_metrics.json        # CACR, R_H, EBE details
│   │   ├── l2_macro_metrics.json        # EPI, benchmark details
│   │   ├── benchmark_comparison.csv     # 8 benchmarks vs empirical
│   │   └── action_distribution.csv      # Per-agent-type action counts
│   │
│   └── paper3_primary/                   # Primary experiment traces
│       ├── seed_42/
│       │   └── gemma3_4b_strict/
│       │       └── raw/
│       │           ├── household_owner_traces.jsonl
│       │           ├── household_renter_traces.jsonl
│       │           ├── government_traces.jsonl
│       │           └── insurance_traces.jsonl
│       ├── seed_43/
│       │   └── ...
│       └── aggregated/                   # Cross-seed aggregation
│           ├── all_owner_traces.jsonl
│           ├── all_renter_traces.jsonl
│           └── seed_summary.csv
│
└── configs/                              # Experiment configurations
    ├── icc_archetypes.yaml              # 15 archetypes for L3
    ├── vignettes/                        # 6 vignettes for L3
    └── calibration.yaml                  # 3-stage protocol
```

## Output File Specifications

### L3 Cognitive Validation (Pre-Experiment)

| File | Generated By | Content |
|------|-------------|---------|
| `cv/icc_report.json` | `run_cv.py --mode icc` | ICC(2,1) TP/CP, eta², pass status |
| `cv/icc_responses.csv` | `run_cv.py --mode icc` | All 2,700 responses with columns: archetype, vignette, replicate, TP, CP, decision, reasoning |
| `cv/persona_sensitivity_report.json` | `run_cv.py --mode persona` | 4 swap test results, chi-squared stats |
| `cv/prompt_sensitivity_report.json` | `run_cv.py --mode prompt` | Option reordering, framing effect tests |

### L1/L2 Validation (Post-Experiment)

| File | Generated By | Content |
|------|-------------|---------|
| `validation/validation_report.json` | `compute_validation_metrics.py` | Combined summary with pass/fail |
| `validation/l1_micro_metrics.json` | `compute_validation_metrics.py` | CACR, R_H, EBE for owner/renter |
| `validation/l2_macro_metrics.json` | `compute_validation_metrics.py` | EPI, 8 benchmark values and ranges |
| `validation/benchmark_comparison.csv` | `compute_validation_metrics.py` | Tabular format for paper |

### Experiment Traces

| File | Content | Size Estimate |
|------|---------|---------------|
| `household_owner_traces.jsonl` | 200 owners × 13 years | ~2,600 lines |
| `household_renter_traces.jsonl` | 200 renters × 13 years | ~2,600 lines |
| `government_traces.jsonl` | 1 NJDEP × 13 years | ~13 lines |
| `insurance_traces.jsonl` | 1 FEMA × 13 years | ~13 lines |

### Figures

| File | Script | Purpose |
|------|--------|---------|
| `figures/agent_spatial_distribution.png` | `fig_agent_spatial_distribution.py` | 400 agents + flood depth + Census Tract boundaries |
| `figures/agent_spatial_distribution.pdf` | Same | Vector for publication |
| `figures/agent_spatial_distribution_panels.png` | Same | 4-panel by agent type with tract boundaries |
| `figures/agent_distribution_stats.csv` | Same | Zone distribution statistics |

## Path Constants (Python)

```python
# In any analysis script:
from pathlib import Path

PAPER3_DIR = Path(__file__).parent.parent  # examples/multi_agent/flood/paper3
FLOOD_DIR = PAPER3_DIR.parent              # examples/multi_agent/flood

# Output directories
FIGURES_DIR = PAPER3_DIR / "analysis" / "figures"
TABLES_DIR = PAPER3_DIR / "analysis" / "tables"
RESULTS_DIR = PAPER3_DIR / "results"
CV_DIR = RESULTS_DIR / "cv"
VALIDATION_DIR = RESULTS_DIR / "validation"
PRIMARY_DIR = RESULTS_DIR / "paper3_primary"

# Input data
DATA_DIR = FLOOD_DIR / "data"
AGENT_PROFILES = DATA_DIR / "agent_profiles_balanced.csv"
INITIAL_MEMORIES = DATA_DIR / "initial_memories_balanced.json"
PRB_DIR = FLOOD_DIR / "input" / "PRB"
CENSUS_DIR = FLOOD_DIR / "input" / "census"
```

## External Data Sources

### Census Tract Boundaries

Census tract shapefiles are automatically downloaded from TIGER/Line when needed:

| File | Source | Description |
|------|--------|-------------|
| `input/census/tl_2023_34_tract.shp` | [TIGER/Line 2023](https://www2.census.gov/geo/tiger/TIGER2023/TRACT/) | NJ Census Tract boundaries |
| `input/census/tl_2023_34_tract.dbf` | Same | Attribute data |
| `input/census/tl_2023_34_tract.prj` | Same | Projection info (NAD83) |
| `input/census/tl_2023_34_tract.shx` | Same | Spatial index |

**Auto-download**: The `fig_agent_spatial_distribution.py` script will automatically download the shapefile if not present.

**Manual download**: Visit https://www2.census.gov/geo/tiger/TIGER2023/TRACT/tl_2023_34_tract.zip

## Validation Workflow Commands

```bash
cd examples/multi_agent/flood

# Step 1: L3 Cognitive Validation (pre-experiment)
python paper3/run_cv.py --mode icc --model gemma3:4b
# Output: paper3/results/cv/icc_report.json, icc_responses.csv

python paper3/run_cv.py --mode persona_sensitivity --model gemma3:4b
# Output: paper3/results/cv/persona_sensitivity_report.json

python paper3/run_cv.py --mode prompt_sensitivity --model gemma3:4b
# Output: paper3/results/cv/prompt_sensitivity_report.json

# Step 2: Primary Experiment
python paper3/run_experiment.py --seed 42 --model gemma3:4b
# Output: paper3/results/paper3_primary/seed_42/...

# Step 3: L1/L2 Validation (post-experiment)
python paper3/analysis/compute_validation_metrics.py \
    --traces paper3/results/paper3_primary/seed_42
# Output: paper3/results/validation/*.json, *.csv

# Step 4: Generate Figures
python paper3/analysis/fig_agent_spatial_distribution.py
# Output: paper3/analysis/figures/*.png, *.pdf

# Step 5: Generate Tables
python paper3/analysis/generate_wrr_tables.py
# Output: paper3/analysis/tables/*.csv
```

## Notes

1. **All paths are relative to `examples/multi_agent/flood/`**
2. **Figures use 300 DPI PNG + PDF for publication**
3. **JSON files use UTF-8 encoding with indent=2**
4. **CSV files use UTF-8 encoding with header row**
5. **JSONL files have one JSON object per line**
