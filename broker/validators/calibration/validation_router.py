"""
Validation Router — Decision-Tree Feature Detection & Routing.

Inspects agent configuration (``agent_types.yaml``) and simulation data
to automatically determine which C&V validators are applicable, then
routes to appropriate validation methods based on available features.

5 Core Metrics (simplified from 16)::

    Level 1 (MICRO):
        CACR — Construct-Action Coherence Rate
        RH   — Hallucination Rate (R_H + EBE)

    Level 2 (MACRO):
        BRC  — Behavioral Reference Concordance (vs. traditional ABM)
        (+ optional: KS/Wasserstein/chi² when N >= 200)

    Level 3 (COGNITIVE):
        ICC  — Intraclass Correlation Coefficient (test-retest)
        EBE  — Effective Behavioral Entropy

Optional diagnostics (kept but not core)::

    ACTION_STABILITY — construct-free temporal stability
    TCS — Temporal Consistency Score
    DISTRIBUTION_MATCH — KS/Wasserstein/chi² (N-dependent)

Literature grounding:
    Grimm et al. (2005) — few metrics at different hierarchical levels
    Windrum et al. (2007) — output validation vs. calibrated reference
    Huang et al. (2025) — LLM psychometric reliability (Nature MI)
    Shannon (1948) — entropy-based diversity measurement

Part of SAGE C&V Framework (feature/calibration-validation).
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Set

import pandas as pd


# ---------------------------------------------------------------------------
# Validator type registry
# ---------------------------------------------------------------------------

class ValidatorType(str, Enum):
    """Available validator types across all three levels.

    5 core metrics (always reported when applicable):
        CACR, RH, BRC, ICC, EBE

    Optional diagnostics (reported when conditions met):
        ACTION_STABILITY, TCS, DISTRIBUTION_MATCH
    """
    # --- Core metrics ---
    CACR = "cacr"                       # L1: Construct-Action Coherence Rate
    RH = "rh"                           # L1: Hallucination Rate
    BRC = "brc"                         # L2: Behavioral Reference Concordance
    ICC = "icc"                         # L3: Test-retest reliability (ICC 2,1)
    EBE = "ebe"                         # L3: Effective Behavioral Entropy

    # --- Optional diagnostics ---
    ACTION_STABILITY = "action_stability"   # Construct-free temporal
    TCS = "tcs"                             # Temporal Consistency Score
    DISTRIBUTION_MATCH = "distribution_match"  # KS/Wasserstein/chi² (N >= 200)


# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------

@dataclass
class ValidatorSpec:
    """Specification for a single validator to run.

    Attributes:
        type: Which validator to use.
        config: Parameters passed to the validator.
        reason: Human-readable explanation of *why* this was selected.
        required: If True, failure to run is an error (not just a skip).
    """
    type: ValidatorType
    config: Dict[str, Any] = field(default_factory=dict)
    reason: str = ""
    required: bool = False


@dataclass
class ValidationPlan:
    """Complete validation plan generated by the router.

    Attributes:
        level1_micro: Level 1 validators to run.
        level2_macro: Level 2 validators to run.
        level3_cognitive: Level 3 validators to run (always non-empty).
        profile: Feature profile used to generate this plan.
    """
    level1_micro: List[ValidatorSpec] = field(default_factory=list)
    level2_macro: List[ValidatorSpec] = field(default_factory=list)
    level3_cognitive: List[ValidatorSpec] = field(default_factory=list)
    profile: Optional[FeatureProfile] = None

    @property
    def all_validators(self) -> List[ValidatorSpec]:
        """Flat list of all planned validators."""
        return self.level1_micro + self.level2_macro + self.level3_cognitive

    def summary(self) -> Dict[str, Any]:
        """Serializable summary of the plan."""
        return {
            "level1": [v.type.value for v in self.level1_micro],
            "level2": [v.type.value for v in self.level2_macro],
            "level3": [v.type.value for v in self.level3_cognitive],
            "total_validators": len(self.all_validators),
            "features_detected": (
                self.profile.to_dict() if self.profile else {}
            ),
        }


@dataclass
class FeatureProfile:
    """Detected features from agent configuration and/or data.

    Built by :meth:`ValidationRouter.detect_features` by inspecting
    the ``agent_types.yaml`` config dict and/or a simulation DataFrame.
    """
    # --- Construct features ---
    has_constructs: bool = False
    construct_names: List[str] = field(default_factory=list)
    construct_cols: Dict[str, str] = field(default_factory=dict)

    # --- Scale features ---
    has_ordinal_scale: bool = False
    ordinal_labels: List[str] = field(default_factory=list)
    label_order: Dict[str, int] = field(default_factory=dict)

    # --- Governance features ---
    has_thinking_rules: bool = False
    has_identity_rules: bool = False
    n_thinking_rules: int = 0
    n_identity_rules: int = 0

    # --- Data features ---
    has_reasoning: bool = False
    reasoning_col: str = ""
    has_temporal: bool = False
    n_years: int = 0
    has_actions: bool = False
    action_list: List[str] = field(default_factory=list)
    decision_col: str = ""

    # --- Multi-agent features ---
    has_multi_agent_types: bool = False
    agent_types: List[str] = field(default_factory=list)
    n_agents: int = 0

    # --- Reference data ---
    has_reference_data: bool = False

    # --- Framework ---
    framework_name: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "has_constructs": self.has_constructs,
            "construct_names": self.construct_names,
            "has_ordinal_scale": self.has_ordinal_scale,
            "has_thinking_rules": self.has_thinking_rules,
            "has_identity_rules": self.has_identity_rules,
            "n_thinking_rules": self.n_thinking_rules,
            "n_identity_rules": self.n_identity_rules,
            "has_reasoning": self.has_reasoning,
            "reasoning_col": self.reasoning_col,
            "has_temporal": self.has_temporal,
            "n_years": self.n_years,
            "has_actions": self.has_actions,
            "n_actions": len(self.action_list),
            "has_multi_agent_types": self.has_multi_agent_types,
            "n_agents": self.n_agents,
            "has_reference_data": self.has_reference_data,
            "framework_name": self.framework_name,
        }


# ---------------------------------------------------------------------------
# Validation Router
# ---------------------------------------------------------------------------

class ValidationRouter:
    """Decision-tree router that plans validation based on detected features.

    Usage::

        profile = ValidationRouter.detect_features(config=cfg, df=df)
        plan = ValidationRouter.plan(profile)

        # Or in one call:
        plan = ValidationRouter.detect_and_plan(config=cfg, df=df)

        # Inspect plan
        print(plan.summary())
        for v in plan.level3_cognitive:
            print(f"  L3: {v.type.value} — {v.reason}")
    """

    # Known ordinal scales for auto-detection
    KNOWN_SCALES: Dict[str, Dict[str, int]] = {
        "pmt_5level": {"VL": 0, "L": 1, "M": 2, "H": 3, "VH": 4},
        "utility_3level": {"L": 0, "M": 1, "H": 2},
        "financial_3level": {"C": 0, "M": 1, "A": 2},
    }

    # Known construct → possible column names in DataFrame
    KNOWN_CONSTRUCT_COLS: Dict[str, List[str]] = {
        "TP_LABEL": ["ta_level", "threat_appraisal", "threat_perception"],
        "CP_LABEL": ["ca_level", "coping_appraisal", "coping_perception"],
        "WSA_LABEL": ["wsa_level", "water_scarcity_assessment"],
        "ACA_LABEL": ["aca_level", "adaptive_capacity_assessment"],
        "SP_LABEL": ["sp_level", "stakeholder_perception"],
        "SC_LABEL": ["sc_level", "social_capital"],
        "PA_LABEL": ["pa_level", "place_attachment"],
        "BUDGET_UTIL": ["budget_util"],
        "EQUITY_GAP": ["equity_gap"],
        "LOSS_RATIO": ["loss_ratio"],
        "SOLVENCY": ["solvency"],
    }

    # ---------------------------------------------------------------
    # Feature detection
    # ---------------------------------------------------------------

    @classmethod
    def detect_features(
        cls,
        config: Optional[Dict[str, Any]] = None,
        df: Optional[pd.DataFrame] = None,
        reference_data: Optional[Dict[str, Any]] = None,
    ) -> FeatureProfile:
        """Detect available features from config and/or data.

        When both config and data are provided, data-level detection
        takes priority for column names (since the DataFrame has the
        actual columns that validators will operate on).

        Parameters
        ----------
        config : dict, optional
            Agent type configuration (from ``agent_types.yaml``).
        df : DataFrame, optional
            Simulation trace data.
        reference_data : dict, optional
            Empirical reference data for macro calibration.

        Returns
        -------
        FeatureProfile
        """
        profile = FeatureProfile()

        if config:
            cls._detect_from_config(config, profile)
        if df is not None:
            # Data detection runs second and overrides column names
            # from config since the DataFrame has actual columns.
            cls._detect_from_data(df, profile)
        if reference_data:
            profile.has_reference_data = True

        return profile

    @classmethod
    def _detect_from_config(
        cls,
        config: Dict[str, Any],
        profile: FeatureProfile,
    ) -> None:
        """Detect features from agent_types.yaml config dict."""

        # ----------------------------------------------------------
        # Walk all top-level keys looking for agent type definitions
        # ----------------------------------------------------------
        reserved_keys = {
            "global_config", "shared", "governance",
            "rating_scale", "response_format",
        }

        # Check shared response_format first
        shared = config.get("shared", {})
        cls._detect_response_format(
            shared.get("response_format", {}), profile
        )

        # Check shared rating_scale
        scale_text = shared.get("rating_scale", "")
        if isinstance(scale_text, str) and "VL" in scale_text and "VH" in scale_text:
            profile.has_ordinal_scale = True
            profile.ordinal_labels = ["VL", "L", "M", "H", "VH"]
            profile.label_order = {"VL": 0, "L": 1, "M": 2, "H": 3, "VH": 4}

        agent_type_names: List[str] = []

        for key, value in config.items():
            if key in reserved_keys or not isinstance(value, dict):
                continue

            agent_type_names.append(key)

            # --- Constructs from parsing.constructs ---
            parsing = value.get("parsing", {})
            constructs = parsing.get("constructs", {})
            if constructs and isinstance(constructs, dict):
                profile.has_constructs = True
                for cname in constructs:
                    if cname not in profile.construct_names:
                        profile.construct_names.append(cname)

            # --- Response format per agent type ---
            cls._detect_response_format(
                value.get("response_format", {}), profile
            )

            # --- Governance rules ---
            governance = value.get("governance", {})
            cls._detect_governance_rules(governance, profile)

            # --- Actions ---
            actions = value.get("actions", [])
            if actions:
                profile.has_actions = True
                for a in actions:
                    aid = a.get("id", a) if isinstance(a, dict) else str(a)
                    if aid not in profile.action_list:
                        profile.action_list.append(aid)

            # Actions from skill_map
            skill_map = parsing.get("skill_map", {})
            if skill_map:
                profile.has_actions = True
                for skill_name in skill_map.values():
                    if skill_name not in profile.action_list:
                        profile.action_list.append(skill_name)

        # --- Top-level governance config ---
        top_governance = config.get("governance", {})
        if isinstance(top_governance, dict):
            cls._detect_governance_rules(top_governance, profile)

        # --- Multi-agent types ---
        if len(agent_type_names) > 1:
            profile.has_multi_agent_types = True
            profile.agent_types = agent_type_names

        # --- Infer framework from constructs ---
        cls._infer_framework(profile)

    @classmethod
    def _detect_response_format(
        cls,
        rf: Dict[str, Any],
        profile: FeatureProfile,
    ) -> None:
        """Extract features from a response_format block."""
        if not rf or not isinstance(rf, dict):
            return

        fields = rf.get("fields", [])
        for f in fields:
            if not isinstance(f, dict):
                continue
            ftype = f.get("type", "")
            fkey = f.get("key", "")

            if ftype == "appraisal" and f.get("construct"):
                profile.has_constructs = True
                cname = f["construct"]
                if cname not in profile.construct_names:
                    profile.construct_names.append(cname)

            if ftype == "text":
                profile.has_reasoning = True
                if not profile.reasoning_col:
                    profile.reasoning_col = fkey

            if ftype == "choice":
                if not profile.decision_col:
                    profile.decision_col = fkey

    @classmethod
    def _detect_governance_rules(
        cls,
        governance: Dict[str, Any],
        profile: FeatureProfile,
    ) -> None:
        """Extract governance rule counts from a governance block."""
        if not isinstance(governance, dict):
            return

        for gov_profile_name, gov_profile in governance.items():
            if not isinstance(gov_profile, dict):
                continue

            # Direct thinking_rules / identity_rules
            thinking = gov_profile.get("thinking_rules", [])
            identity = gov_profile.get("identity_rules", [])
            if thinking and isinstance(thinking, list):
                profile.has_thinking_rules = True
                profile.n_thinking_rules = max(
                    profile.n_thinking_rules, len(thinking)
                )
            if identity and isinstance(identity, list):
                profile.has_identity_rules = True
                profile.n_identity_rules = max(
                    profile.n_identity_rules, len(identity)
                )

            # Nested per-agent-type rules
            for sub_key, sub_val in gov_profile.items():
                if isinstance(sub_val, dict):
                    sub_thinking = sub_val.get("thinking_rules", [])
                    sub_identity = sub_val.get("identity_rules", [])
                    if sub_thinking and isinstance(sub_thinking, list):
                        profile.has_thinking_rules = True
                        profile.n_thinking_rules = max(
                            profile.n_thinking_rules, len(sub_thinking)
                        )
                    if sub_identity and isinstance(sub_identity, list):
                        profile.has_identity_rules = True
                        profile.n_identity_rules = max(
                            profile.n_identity_rules, len(sub_identity)
                        )

    @classmethod
    def _infer_framework(cls, profile: FeatureProfile) -> None:
        """Infer psychological framework from detected constructs."""
        if not profile.construct_names:
            return

        cset = set(profile.construct_names)

        if {"TP_LABEL", "CP_LABEL"}.issubset(cset):
            profile.framework_name = "pmt"
        elif {"WSA_LABEL", "ACA_LABEL"}.issubset(cset):
            profile.framework_name = "dual_appraisal"
        elif "BUDGET_UTIL" in cset:
            profile.framework_name = "utility"
        elif "LOSS_RATIO" in cset:
            profile.framework_name = "financial"

    @classmethod
    def _detect_from_data(
        cls,
        df: pd.DataFrame,
        profile: FeatureProfile,
    ) -> None:
        """Detect features from simulation DataFrame columns."""
        cols = set(df.columns)

        # --- Agent count ---
        if "agent_id" in cols:
            profile.n_agents = int(df["agent_id"].nunique())

        # --- Temporal ---
        if "year" in cols:
            n_years = int(df["year"].nunique())
            if n_years > 1:
                profile.has_temporal = True
                profile.n_years = n_years

        # --- Reasoning ---
        reasoning_candidates = [
            "reasoning", "rationale", "explanation", "thought",
        ]
        for rc in reasoning_candidates:
            if rc in cols:
                profile.has_reasoning = True
                profile.reasoning_col = rc  # Always override from data
                break

        # --- Constructs from data columns ---
        for construct_name, col_candidates in cls.KNOWN_CONSTRUCT_COLS.items():
            for col in col_candidates:
                if col in cols:
                    if construct_name not in profile.construct_names:
                        profile.has_constructs = True
                        profile.construct_names.append(construct_name)
                    profile.construct_cols[construct_name] = col
                    break

        # --- Ordinal scale auto-detection from data values ---
        if not profile.has_ordinal_scale and profile.construct_cols:
            for _cname, col in profile.construct_cols.items():
                unique_vals = set(
                    df[col].dropna().astype(str).str.upper().unique()
                )
                for _scale_name, scale_map in cls.KNOWN_SCALES.items():
                    if unique_vals.issubset(set(scale_map.keys())):
                        profile.has_ordinal_scale = True
                        profile.ordinal_labels = sorted(
                            scale_map.keys(), key=lambda x: scale_map[x]
                        )
                        profile.label_order = dict(scale_map)
                        break
                if profile.has_ordinal_scale:
                    break

        # --- Actions ---
        # Data columns take priority over config-derived column names,
        # because the DataFrame has the actual columns validators use.
        decision_candidates = [
            "yearly_decision", "decision", "action", "skill",
            "chosen_action",
        ]
        for dc in decision_candidates:
            if dc in cols:
                profile.has_actions = True
                profile.decision_col = dc  # Always override from data
                profile.action_list = sorted(
                    df[dc].dropna().astype(str).unique().tolist()
                )
                break

        # --- Multi-agent types ---
        type_candidates = ["agent_type", "agent_category", "role"]
        for tc in type_candidates:
            if tc in cols and df[tc].nunique() > 1:
                profile.has_multi_agent_types = True
                profile.agent_types = sorted(
                    df[tc].unique().tolist()
                )
                break

        # --- Framework inference (if not already set from config) ---
        if not profile.framework_name:
            cls._infer_framework(profile)

    # ---------------------------------------------------------------
    # Plan generation (decision tree)
    # ---------------------------------------------------------------

    @classmethod
    def plan(cls, profile: FeatureProfile) -> ValidationPlan:
        """Generate validation plan from detected feature profile.

        Parameters
        ----------
        profile : FeatureProfile

        Returns
        -------
        ValidationPlan
        """
        return ValidationPlan(
            level1_micro=cls._plan_level1(profile),
            level2_macro=cls._plan_level2(profile),
            level3_cognitive=cls._plan_level3(profile),
            profile=profile,
        )

    # Threshold for enabling distribution matching (KS/Wasserstein/chi²)
    DISTRIBUTION_MATCH_MIN_N: int = 200

    @classmethod
    def _plan_level1(cls, p: FeatureProfile) -> List[ValidatorSpec]:
        """Plan Level 1 MICRO validators.

        Core metrics::

            has_constructs + framework → CACR (M1)
            always (if actions) → RH (M2)

        Optional diagnostics::

            has_temporal + ordinal → TCS
            has_temporal + actions (no ordinal) → Action Stability
        """
        specs: List[ValidatorSpec] = []

        # --- CORE: CACR (M1) ---
        if p.has_constructs and p.framework_name:
            specs.append(ValidatorSpec(
                type=ValidatorType.CACR,
                config={
                    "framework": p.framework_name,
                    "construct_names": p.construct_names,
                    "construct_cols": p.construct_cols,
                },
                reason=(
                    "Psychological constructs detected "
                    f"({', '.join(p.construct_names)}) "
                    f"→ CACR with {p.framework_name} framework"
                ),
                required=True,
            ))

        # --- CORE: RH (M2) ---
        if p.has_actions:
            specs.append(ValidatorSpec(
                type=ValidatorType.RH,
                config={
                    "decision_col": p.decision_col,
                    "has_constructs": p.has_constructs,
                },
                reason=(
                    "R_H measures physical hallucinations + thinking "
                    "violations; EBE = H_norm * (1 - R_H)"
                ),
                required=True,
            ))

        # --- OPTIONAL: Temporal diagnostics ---
        if p.has_temporal:
            if p.has_ordinal_scale and p.has_constructs:
                specs.append(ValidatorSpec(
                    type=ValidatorType.TCS,
                    config={
                        "construct_cols": list(p.construct_cols.values()),
                        "label_order": p.label_order,
                    },
                    reason=(
                        f"Temporal ({p.n_years} years) + ordinal constructs "
                        "→ TCS detects impossible transitions"
                    ),
                ))
            elif p.has_actions:
                specs.append(ValidatorSpec(
                    type=ValidatorType.ACTION_STABILITY,
                    config={
                        "decision_col": p.decision_col,
                        "action_list": p.action_list,
                    },
                    reason=(
                        f"Temporal ({p.n_years} years) + actions (no ordinal "
                        "constructs) → Action Stability Score"
                    ),
                ))

        return specs

    @classmethod
    def _plan_level2(cls, p: FeatureProfile) -> List[ValidatorSpec]:
        """Plan Level 2 MACRO validators.

        Core metric::

            has_constructs + framework → BRC (M3)

        Optional (N-dependent)::

            N >= 200 + reference_data → Distribution Match (KS/Wasserstein)
        """
        specs: List[ValidatorSpec] = []

        # --- CORE: BRC (M3) ---
        if p.has_constructs and p.framework_name and p.has_actions:
            specs.append(ValidatorSpec(
                type=ValidatorType.BRC,
                config={
                    "framework": p.framework_name,
                    "decision_col": p.decision_col,
                    "construct_cols": p.construct_cols,
                },
                reason=(
                    f"Constructs + {p.framework_name} framework → BRC "
                    "compares LLM actions against traditional ABM reference"
                ),
                required=True,
            ))

        # --- OPTIONAL: Distribution matching (N-dependent) ---
        if p.has_reference_data and p.has_actions:
            is_large_n = p.n_agents >= cls.DISTRIBUTION_MATCH_MIN_N
            specs.append(ValidatorSpec(
                type=ValidatorType.DISTRIBUTION_MATCH,
                config={
                    "decision_col": p.decision_col,
                    "n_agents": p.n_agents,
                },
                reason=(
                    f"Reference data available (N={p.n_agents}); "
                    + ("statistically viable → KS/Wasserstein/chi²"
                       if is_large_n
                       else "N < 200 → report as diagnostic only")
                ),
                required=is_large_n,
            ))

        return specs

    @classmethod
    def _plan_level3(cls, p: FeatureProfile) -> List[ValidatorSpec]:
        """Plan Level 3 COGNITIVE validators.

        Core metrics::

            ALWAYS (if actions) → ICC (M4) — test-retest reliability
            ALWAYS (if actions + temporal) → EBE (M5) — behavioral entropy
        """
        specs: List[ValidatorSpec] = []

        # --- CORE: ICC (M4) ---
        if p.has_actions:
            specs.append(ValidatorSpec(
                type=ValidatorType.ICC,
                config={
                    "decision_col": p.decision_col,
                    "action_list": p.action_list,
                    "has_constructs": p.has_constructs,
                    "construct_names": p.construct_names,
                },
                reason=(
                    "ICC(2,1) measures test-retest reliability "
                    f"(decision-level"
                    + (f" + {', '.join(p.construct_names)}"
                       if p.has_constructs else "")
                    + ")"
                ),
                required=True,
            ))

        # --- CORE: EBE (M5) ---
        if p.has_actions and p.has_temporal:
            specs.append(ValidatorSpec(
                type=ValidatorType.EBE,
                config={"decision_col": p.decision_col},
                reason=(
                    "EBE = H_norm * (1 - R_H); measures "
                    "diversity-correctness tradeoff"
                ),
                required=True,
            ))

        return specs

    # ---------------------------------------------------------------
    # Convenience
    # ---------------------------------------------------------------

    @classmethod
    def detect_and_plan(
        cls,
        config: Optional[Dict[str, Any]] = None,
        df: Optional[pd.DataFrame] = None,
        reference_data: Optional[Dict[str, Any]] = None,
    ) -> ValidationPlan:
        """Detect features and generate plan in one call."""
        profile = cls.detect_features(config, df, reference_data)
        return cls.plan(profile)
