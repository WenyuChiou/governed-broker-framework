from datetime import datetime
from dataclasses import dataclass
import json
import csv
from pathlib import Path
from typing import Dict, Any, List, Optional


@dataclass
class AuditConfig:
    """Configuration for audit writing."""
    output_dir: str
    experiment_name: str = "simulation"
    log_level: str = "full"  # full, summary, errors_only


class GenericAuditWriter:
    """
    Generic audit writer for any agent type.
    
    Uses Dict-based traces instead of typed dataclasses.
    Automatically creates per-agent-type files.
    """
    
    def __init__(self, config: AuditConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Track file handles per agent type
        self._files: Dict[str, Path] = {}
        
        # Summary stats per agent type
        self.summary = {
            "experiment_name": config.experiment_name,
            "agent_types": {},
            "total_traces": 0,
            "validation_errors": 0,
            "validation_warnings": 0
        }
        
        # Buffer for CSV export
        self._trace_buffer: Dict[str, List[Dict[str, Any]]] = {}
    
    def _get_file_path(self, agent_type: str) -> Path:
        """Get or create file path for agent type (JSONL traces in raw/ subdir)."""
        if agent_type not in self._files:
            raw_dir = self.output_dir / "raw"
            raw_dir.mkdir(parents=True, exist_ok=True)
            self._files[agent_type] = raw_dir / f"{agent_type}_traces.jsonl"
        return self._files[agent_type]
    
    def write_trace(
        self,
        agent_type: str,
        trace: Dict[str, Any],
        validation_results: Optional[List] = None
    ) -> None:
        """Write a generic trace for any agent type."""
        # Ensure required fields
        trace.setdefault("timestamp", datetime.now().isoformat())
        trace.setdefault("agent_type", agent_type)
        
        # Add basic validation info if provided
        if validation_results:
            trace["validated"] = all(r.valid for r in validation_results)
            trace["validation_issues"] = []
            seen_issues = set()
            for r in validation_results:
                if not r.valid:
                    self.summary["validation_errors"] += 1
                    issue_key = (r.metadata.get("rule_id", "Unknown"), tuple(r.errors))
                    if issue_key not in seen_issues:
                        trace["validation_issues"].append({
                            "validator": getattr(r, 'validator_name', 'Unknown'),
                            "rule_id": r.metadata.get("rule_id", "Unknown"),
                            "errors": r.errors
                        })
                        seen_issues.add(issue_key)
        else:
            trace["validated"] = True
            trace["validation_issues"] = []
        
        # Update summary
        self.summary["total_traces"] += 1
        if agent_type not in self.summary["agent_types"]:
            self.summary["agent_types"][agent_type] = {
                "total": 0,
                "decisions": {}
            }
        self.summary["agent_types"][agent_type]["total"] += 1
        
        # Track decision
        decision = trace.get("decision", trace.get("approved_skill", {}).get("skill_name", "unknown"))
        decisions = self.summary["agent_types"][agent_type]["decisions"]
        decisions[decision] = decisions.get(decision, 0) + 1
        
        # Write JSONL
        file_path = self._get_file_path(agent_type)
        with open(file_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(trace, ensure_ascii=False, default=str) + '\n')
        
        # Buffer for CSV
        if agent_type not in self._trace_buffer:
            self._trace_buffer[agent_type] = []
        self._trace_buffer[agent_type].append(trace)
    
    def finalize(self) -> Dict[str, Any]:
        """Write summary and export CSVs."""
        self.summary["finalized_at"] = datetime.now().isoformat()
        
        # Export summary JSON
        summary_path = self.output_dir / "audit_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(self.summary, f, indent=2, ensure_ascii=False)
        
        # Export CSVs
        for agent_type, traces in self._trace_buffer.items():
            self._export_csv(agent_type, traces)
            
        print(f"[Audit] Finalized. Summary: {summary_path}")
        return self.summary

    def _export_csv(self, agent_type: str, traces: List[Dict[str, Any]]):
        """Export buffered traces to flat CSV with deep governance fields."""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        csv_path = self.output_dir / f"{agent_type}_governance_audit.csv"
        if not traces: return

        flat_rows = []
        for t in traces:
            # 1. Base identity and timing
            row = {
                "step_id": t.get("step_id"),
                "timestamp": t.get("timestamp"),
                "agent_id": t.get("agent_id"),
                "status": t.get("approved_skill", {}).get("status", "UNKNOWN"),
                "retry_count": t.get("retry_count", 0),
                "validated": t.get("validated", True),
            }
            
            # 2. Skill Logic (Proposed vs Approved)
            skill_prop = t.get("skill_proposal", {})
            row["proposed_skill"] = skill_prop.get("skill_name")
            row["final_skill"] = t.get("approved_skill", {}).get("skill_name")
            row["parsing_warnings"] = "|".join(skill_prop.get("parsing_warnings", []))
            row["raw_output"] = skill_prop.get("raw_output", "")
            
            # 3. Reasoning (TP/CP Appraisal)
            reasoning = skill_prop.get("reasoning", {})
            if isinstance(reasoning, dict):
                for k, v in reasoning.items():
                    row[f"reason_{k.lower()}"] = v
            else:
                row["reason_text"] = str(reasoning)
            
            # 4. Validation Details (Which rule triggered)
            issues = t.get("validation_issues", [])
            if issues:
                row["failed_rules"] = "|".join([str(i.get('rule_id', 'Unknown')) for i in issues])
                row["error_messages"] = "|".join(["; ".join(i.get('errors', [])) for i in issues])
            else:
                row["failed_rules"] = ""
                row["error_messages"] = ""

            flat_rows.append(row)

        if not flat_rows: return

        # Ensure consistent column ordering for user
        priority_keys = ["step_id", "agent_id", "proposed_skill", "final_skill", "status", "retry_count", "validated", "failed_rules"]
        all_keys = list(flat_rows[0].keys())
        # Sort keys to keep priority ones first
        fieldnames = priority_keys + [k for k in all_keys if k not in priority_keys]
        
        with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(flat_rows)


# Aliases
AuditWriter = GenericAuditWriter
