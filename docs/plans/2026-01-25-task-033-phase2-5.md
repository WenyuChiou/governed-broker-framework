# Task-033 Phase 2 & 5 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement scalability (PolicyCache + batch_verify) and research reproducibility modules with tests.

**Architecture:** Phase 2 adds an LRU cache for compiled policy rules and batch verification to the PolicyEngine. Phase 5 adds a research module with session metadata and trace export utilities. Each phase is isolated in its own worktree and validated by dedicated tests.

**Tech Stack:** Python 3, pytest, governed_ai_sdk v1_prototype

---

### Task 1: Phase 2 Policy Cache (Scalability)

**Files:**
- Create: `governed_ai_sdk/v1_prototype/core/policy_cache.py`
- Modify: `governed_ai_sdk/v1_prototype/core/engine.py`
- Test: `governed_ai_sdk/tests/test_scalability.py`

**Step 1: Write the failing tests**

```python
# governed_ai_sdk/tests/test_scalability.py
"""Tests for scalability features."""
import pytest
import time
from governed_ai_sdk.v1_prototype.core.engine import PolicyEngine
from governed_ai_sdk.v1_prototype.core.policy_cache import PolicyCache


class TestPolicyCache:
    """Tests for PolicyCache."""

    def test_cache_hit(self):
        """Cached policies return same rules."""
        cache = PolicyCache(max_size=10)
        policy = {"rules": [{"id": "r1", "param": "x", "operator": ">=", "value": 10, "message": "test", "level": "ERROR"}]}

        rules1 = cache.get_or_compile(policy)
        rules2 = cache.get_or_compile(policy)

        assert rules1 is rules2  # Same object (cached)

    def test_cache_eviction(self):
        """LRU eviction works correctly."""
        cache = PolicyCache(max_size=2)

        p1 = {"rules": [{"id": "r1", "param": "x", "operator": ">=", "value": 1, "message": "m", "level": "ERROR"}]}
        p2 = {"rules": [{"id": "r2", "param": "x", "operator": ">=", "value": 2, "message": "m", "level": "ERROR"}]}
        p3 = {"rules": [{"id": "r3", "param": "x", "operator": ">=", "value": 3, "message": "m", "level": "ERROR"}]}

        cache.get_or_compile(p1)
        cache.get_or_compile(p2)
        cache.get_or_compile(p3)  # Should evict p1

        assert cache.stats()["size"] == 2

    def test_severity_sorting(self):
        """Rules are sorted by severity (high first)."""
        cache = PolicyCache()
        policy = {
            "rules": [
                {"id": "r1", "param": "x", "operator": ">=", "value": 1, "message": "low", "level": "ERROR", "severity_score": 0.3},
                {"id": "r2", "param": "x", "operator": ">=", "value": 2, "message": "high", "level": "ERROR", "severity_score": 0.9},
            ]
        }

        rules = cache.get_or_compile(policy)
        assert rules[0].id == "r2"  # High severity first
        assert rules[1].id == "r1"


class TestBatchVerify:
    """Tests for batch verification."""

    def test_batch_verify_sequential(self):
        """Batch verify works sequentially."""
        engine = PolicyEngine()
        policy = {"rules": [{"id": "r1", "param": "x", "operator": ">=", "value": 10, "message": "test", "level": "ERROR"}]}

        requests = [({}, {"x": i}) for i in range(20)]
        results = engine.batch_verify(requests, policy, parallel=False)

        assert len(results) == 20
        assert sum(r.valid for r in results) == 10  # x >= 10

    def test_batch_verify_parallel(self):
        """Batch verify works in parallel."""
        engine = PolicyEngine()
        policy = {"rules": [{"id": "r1", "param": "x", "operator": ">=", "value": 10, "message": "test", "level": "ERROR"}]}

        requests = [({}, {"x": i}) for i in range(100)]
        results = engine.batch_verify(requests, policy, parallel=True)

        assert len(results) == 100
        assert sum(r.valid for r in results) == 90  # x >= 10

    def test_batch_performance(self):
        """Batch processing is faster than sequential for large requests."""
        engine = PolicyEngine()
        policy = {"rules": [{"id": "r1", "param": "x", "operator": ">=", "value": 10, "message": "test", "level": "ERROR"}]}

        requests = [({}, {"x": i % 100}) for i in range(1000)]

        start = time.time()
        engine.batch_verify(requests, policy, parallel=True)
        parallel_time = time.time() - start

        assert parallel_time < 5.0  # Should complete in under 5 seconds
```

**Step 2: Run test to verify it fails**

Run: `python -m pytest governed_ai_sdk/tests/test_scalability.py -v`
Expected: FAIL (missing PolicyCache, batch_verify)

**Step 3: Write minimal implementation**

```python
# governed_ai_sdk/v1_prototype/core/policy_cache.py
from collections import OrderedDict
from typing import Dict, List, Any
from ..types import PolicyRule


class PolicyCache:
    """LRU cache for compiled policy rules."""

    def __init__(self, max_size: int = 100):
        self._cache: OrderedDict[str, List[PolicyRule]] = OrderedDict()
        self._max_size = max_size

    def get_or_compile(self, policy: Dict[str, Any]) -> List[PolicyRule]:
        policy_id = self._compute_hash(policy)
        if policy_id in self._cache:
            self._cache.move_to_end(policy_id)
            return self._cache[policy_id]

        rules = self._compile_rules(policy)
        self._cache[policy_id] = rules
        if len(self._cache) > self._max_size:
            self._cache.popitem(last=False)
        return rules

    def _compute_hash(self, policy: Dict[str, Any]) -> str:
        import hashlib
        import json
        policy_str = json.dumps(policy, sort_keys=True)
        return hashlib.md5(policy_str.encode()).hexdigest()

    def _compile_rules(self, policy: Dict[str, Any]) -> List[PolicyRule]:
        rules = []
        for r in policy.get("rules", []):
            rule = PolicyRule(
                id=r["id"],
                param=r["param"],
                operator=r["operator"],
                value=r["value"],
                message=r["message"],
                level=r.get("level", "ERROR"),
                xai_hint=r.get("xai_hint"),
                domain=r.get("domain", "generic"),
                param_type=r.get("param_type", "numeric"),
                param_unit=r.get("param_unit"),
                severity_score=r.get("severity_score", 1.0),
                literature_ref=r.get("literature_ref"),
                rationale=r.get("rationale"),
            )
            rules.append(rule)
        return sorted(rules, key=lambda r: r.severity_score, reverse=True)

    def clear(self):
        self._cache.clear()

    def stats(self) -> Dict[str, int]:
        return {"size": len(self._cache), "max_size": self._max_size}
```

```python
# governed_ai_sdk/v1_prototype/core/engine.py (PolicyEngine additions)
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple
from .policy_cache import PolicyCache

class PolicyEngine:
    def __init__(self, cache_size: int = 100):
        self._cache = PolicyCache(max_size=cache_size)

    def batch_verify(self, requests, policy, parallel=True, max_workers=4):
        if parallel and len(requests) > 10:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                return list(executor.map(lambda req: self.verify(req[0], req[1], policy), requests))
        return [self.verify(action, state, policy) for action, state in requests]
```

**Step 4: Run test to verify it passes**

Run: `python -m pytest governed_ai_sdk/tests/test_scalability.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add governed_ai_sdk/v1_prototype/core/policy_cache.py governed_ai_sdk/v1_prototype/core/engine.py governed_ai_sdk/tests/test_scalability.py
git commit -m "feat(sdk): add policy cache and batch verify"
```

---

### Task 2: Phase 5 Research Module

**Files:**
- Create: `governed_ai_sdk/v1_prototype/research/__init__.py`
- Create: `governed_ai_sdk/v1_prototype/research/session.py`
- Create: `governed_ai_sdk/v1_prototype/research/export.py`
- Test: `governed_ai_sdk/tests/test_research.py`

**Step 1: Write the failing tests**

```python
# governed_ai_sdk/tests/test_research.py
"""Tests for research reproducibility features."""
import pytest
import tempfile
import os
from datetime import datetime
from governed_ai_sdk.v1_prototype.research.session import ResearchSession
from governed_ai_sdk.v1_prototype.research.export import (
    export_traces_to_csv,
    export_to_json,
    export_summary_stats,
)


class MockTrace:
    """Mock trace object for testing."""
    def __init__(self, valid=True, treatment_group="control", **kwargs):
        self.trace_id = kwargs.get("trace_id", "t1")
        self.timestamp = kwargs.get("timestamp", datetime.now())
        self.valid = valid
        self.decision = kwargs.get("decision", "allow")
        self.blocked_by = kwargs.get("blocked_by")
        self.treatment_group = treatment_group
        self.domain = kwargs.get("domain", "flood")
        self.research_phase = kwargs.get("research_phase", "main_study")


class TestResearchSession:
    """Tests for ResearchSession."""

    def test_create_session(self):
        session = ResearchSession(
            study_id="FLOOD-2024-001",
            domain="flood",
            protocol_version="1.0",
            seed=42,
            treatment_groups={"control": 50, "treatment": 50},
            total_agents=100,
        )
        assert session.study_id == "FLOOD-2024-001"
        assert session.total_agents == 100

    def test_to_methods_section(self):
        session = ResearchSession(
            study_id="TEST-001",
            domain="flood",
            protocol_version="1.0",
            treatment_groups={"control": 10, "treatment": 10},
            total_agents=20,
        )
        methods = session.to_methods_section()
        assert "## Methods" in methods
        assert "flood" in methods
        assert "N=20" in methods

    def test_save_load(self):
        session = ResearchSession(
            study_id="TEST-002",
            domain="finance",
            protocol_version="2.0",
            seed=123,
        )

        with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as f:
            path = f.name

        try:
            session.save(path)
            loaded = ResearchSession.load(path)
            assert loaded.study_id == "TEST-002"
            assert loaded.seed == 123
        finally:
            os.unlink(path)

    def test_to_dict_roundtrip(self):
        session = ResearchSession(
            study_id="TEST-003",
            domain="health",
            protocol_version="1.0",
        )
        data = session.to_dict()
        restored = ResearchSession.from_dict(data)
        assert restored.study_id == session.study_id
        assert restored.domain == session.domain


class TestExport:
    """Tests for export functions."""

    def test_export_csv(self):
        traces = [
            MockTrace(valid=True, treatment_group="control"),
            MockTrace(valid=False, treatment_group="treatment", blocked_by="r1"),
        ]

        with tempfile.NamedTemporaryFile(suffix=".csv", delete=False) as f:
            path = f.name

        try:
            export_traces_to_csv(traces, path)
            with open(path) as f:
                content = f.read()
            assert "valid" in content
            assert "treatment_group" in content
        finally:
            os.unlink(path)

    def test_export_json(self):
        traces = [
            MockTrace(valid=True),
            MockTrace(valid=False),
        ]

        with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as f:
            path = f.name

        try:
            export_to_json(traces, path)
            import json
            with open(path) as f:
                data = json.load(f)
            assert len(data) == 2
        finally:
            os.unlink(path)

    def test_summary_stats(self):
        traces = [
            MockTrace(valid=True, treatment_group="control"),
            MockTrace(valid=True, treatment_group="control"),
            MockTrace(valid=False, treatment_group="treatment"),
        ]

        with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as f:
            path = f.name

        try:
            stats = export_summary_stats(traces, path)
            assert stats["total_traces"] == 3
            assert stats["valid_count"] == 2
            assert stats["blocked_count"] == 1
            assert stats["by_treatment_group"]["control"]["valid"] == 2
        finally:
            os.unlink(path)
```

**Step 2: Run test to verify it fails**

Run: `python -m pytest governed_ai_sdk/tests/test_research.py -v`
Expected: FAIL (missing research module)

**Step 3: Write minimal implementation**

```python
# governed_ai_sdk/v1_prototype/research/__init__.py
from .session import ResearchSession
from .export import export_traces_to_csv, export_to_stata, export_to_json

__all__ = [
    "ResearchSession",
    "export_traces_to_csv",
    "export_to_stata",
    "export_to_json",
]
```

```python
# governed_ai_sdk/v1_prototype/research/session.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, Any, Optional
import json


@dataclass
class ResearchSession:
    study_id: str
    domain: str
    protocol_version: str
    execution_date: datetime = field(default_factory=datetime.now)
    seed: int = 42
    treatment_groups: Dict[str, int] = field(default_factory=dict)
    sensor_schemas: Dict[str, Any] = field(default_factory=dict)
    total_agents: int = 0
    framework_version: str = "0.1.0"

    researcher: Optional[str] = None
    institution: Optional[str] = None
    irb_number: Optional[str] = None
    notes: Optional[str] = None

    def to_methods_section(self) -> str:
        groups_str = ", ".join(f"{k}={v}" for k, v in self.treatment_groups.items())
        return f"""## Methods

### Simulation Framework
- **Framework**: GovernedAI SDK v{self.framework_version}
- **Domain**: {self.domain}
- **Protocol Version**: {self.protocol_version}

### Participants
- **Total Agents**: N={self.total_agents}
- **Treatment Groups**: {groups_str}

### Reproducibility
- **Study ID**: {self.study_id}
- **Random Seed**: {self.seed}
- **Execution Date**: {self.execution_date.strftime('%Y-%m-%d %H:%M:%S')}

### Sensor Configuration
{self._format_sensors()}
"""

    def _format_sensors(self) -> str:
        if not self.sensor_schemas:
            return "No sensors configured."

        lines = ["| Sensor | Variable | Data Type | Units |", "|--------|----------|-----------|-------|"]
        for name, schema in self.sensor_schemas.items():
            var = schema.get("variable_name", "N/A")
            dtype = schema.get("data_type", "N/A")
            units = schema.get("units", "N/A")
            lines.append(f"| {name} | {var} | {dtype} | {units} |")

        return "\n".join(lines)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "study_id": self.study_id,
            "domain": self.domain,
            "protocol_version": self.protocol_version,
            "execution_date": self.execution_date.isoformat(),
            "seed": self.seed,
            "treatment_groups": self.treatment_groups,
            "sensor_schemas": self.sensor_schemas,
            "total_agents": self.total_agents,
            "framework_version": self.framework_version,
            "researcher": self.researcher,
            "institution": self.institution,
            "irb_number": self.irb_number,
            "notes": self.notes,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ResearchSession":
        data = data.copy()
        if isinstance(data.get("execution_date"), str):
            data["execution_date"] = datetime.fromisoformat(data["execution_date"])
        return cls(**data)

    def save(self, path: str) -> None:
        with open(path, "w") as f:
            json.dump(self.to_dict(), f, indent=2)

    @classmethod
    def load(cls, path: str) -> "ResearchSession":
        with open(path, "r") as f:
            return cls.from_dict(json.load(f))
```

```python
# governed_ai_sdk/v1_prototype/research/export.py
from typing import List, Dict, Any
import json


def _trace_to_research_dict(trace: Any) -> Dict[str, Any]:
    result = {
        "trace_id": getattr(trace, "trace_id", None),
        "timestamp": str(getattr(trace, "timestamp", "")),
        "valid": getattr(trace, "valid", None),
        "decision": getattr(trace, "decision", None),
        "blocked_by": getattr(trace, "blocked_by", None),
    }

    if hasattr(trace, "domain"):
        result["domain"] = trace.domain
    if hasattr(trace, "research_phase"):
        result["research_phase"] = trace.research_phase
    if hasattr(trace, "treatment_group"):
        result["treatment_group"] = trace.treatment_group
    if hasattr(trace, "effect_size"):
        result["effect_size"] = trace.effect_size
    if hasattr(trace, "baseline_surprise"):
        result["baseline_surprise"] = trace.baseline_surprise

    delta_state = getattr(trace, "delta_state", None)
    if delta_state and isinstance(delta_state, dict):
        for k, v in delta_state.items():
            result[f"delta_{k}"] = v

    cf = getattr(trace, "counterfactual", None)
    if cf:
        result["cf_feasibility"] = getattr(cf, "feasibility_score", None)
        result["cf_strategy"] = str(getattr(cf, "strategy_used", ""))

    return result


def export_traces_to_csv(traces: List[Any], path: str, include_headers: bool = True) -> None:
    if not traces:
        return

    rows = [_trace_to_research_dict(t) for t in traces]

    all_keys = set()
    for row in rows:
        all_keys.update(row.keys())
    all_keys = sorted(all_keys)

    with open(path, "w", newline="", encoding="utf-8") as f:
        if include_headers:
            f.write(",".join(all_keys) + "\n")

        for row in rows:
            values = []
            for key in all_keys:
                val = row.get(key, "")
                if val is None:
                    val = ""
                elif isinstance(val, bool):
                    val = "1" if val else "0"
                elif isinstance(val, (int, float)):
                    val = str(val)
                else:
                    val = str(val).replace('"', '""')
                    if "," in val or '"' in val or "\n" in val:
                        val = f'"{val}"'
                values.append(val)
            f.write(",".join(values) + "\n")


def export_to_stata(traces: List[Any], path: str) -> None:
    try:
        import pandas as pd
        rows = [_trace_to_research_dict(t) for t in traces]
        df = pd.DataFrame(rows)
        df.to_stata(path, write_index=False)
    except ImportError:
        csv_path = path.replace(".dta", ".csv")
        export_traces_to_csv(traces, csv_path)
        raise ImportError(
            f"pandas with pyreadstat required for Stata export. Saved as CSV instead: {csv_path}"
        )


def export_to_json(traces: List[Any], path: str, indent: int = 2) -> None:
    rows = [_trace_to_research_dict(t) for t in traces]
    with open(path, "w", encoding="utf-8") as f:
        json.dump(rows, f, indent=indent, default=str)


def export_summary_stats(traces: List[Any], path: str) -> Dict[str, Any]:
    if not traces:
        return {}

    rows = [_trace_to_research_dict(t) for t in traces]

    total = len(rows)
    valid_count = sum(1 for r in rows if r.get("valid"))
    blocked_count = total - valid_count

    by_treatment = {}
    for row in rows:
        group = row.get("treatment_group", "unknown")
        if group not in by_treatment:
            by_treatment[group] = {"total": 0, "valid": 0, "blocked": 0}
        by_treatment[group]["total"] += 1
        if row.get("valid"):
            by_treatment[group]["valid"] += 1
        else:
            by_treatment[group]["blocked"] += 1

    stats = {
        "total_traces": total,
        "valid_count": valid_count,
        "blocked_count": blocked_count,
        "valid_rate": valid_count / total if total > 0 else 0,
        "by_treatment_group": by_treatment,
    }

    with open(path, "w") as f:
        json.dump(stats, f, indent=2)

    return stats
```

**Step 4: Run test to verify it passes**

Run: `python -m pytest governed_ai_sdk/tests/test_research.py -v`
Expected: PASS

**Step 5: Commit**

```bash
git add governed_ai_sdk/v1_prototype/research/__init__.py governed_ai_sdk/v1_prototype/research/session.py governed_ai_sdk/v1_prototype/research/export.py governed_ai_sdk/tests/test_research.py
git commit -m "feat(sdk): add research reproducibility module"
```

---

Plan complete and saved to `docs/plans/2026-01-25-task-033-phase2-5.md`.

Two execution options:

1. Subagent-Driven (this session)
2. Parallel Session (separate)

Which approach?
